{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import boto3\n",
    "import random\n",
    "import os\n",
    "import config\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gzip\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['click']\n",
    "LABEL = 'click'\n",
    "CSV_SEPARATOR = '\\t'\n",
    "DATA_STATS_FILE_KEY='trainer_predict_imp/data/stats_click.json'\n",
    "S3_BUCKET = 'wsbidder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(bucket, prefix):\n",
    "    for obj in boto3.resource('s3').Bucket(bucket).objects.filter(Prefix=prefix).all():\n",
    "        obj.delete()\n",
    "\n",
    "def get_files(bucket, prefix, days=[], hours=[], ext='.csv'):\n",
    "    all_files = []\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    for day in days:\n",
    "        for hour in hours:\n",
    "            _prefix = '{}/d={}/h={}/'.format(prefix, day, hour)\n",
    "            new_files = [ obj.key for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=_prefix).all() if obj.key.endswith(ext) ]\n",
    "            all_files = all_files + new_files\n",
    "            \n",
    "    return all_files\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats_and_upload_to_s3(all_files):\n",
    "    CONFIG = config.get_config()\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    df = pd.concat((pd.read_csv(f, sep=CSV_SEPARATOR, compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes) for f in all_files))\n",
    "    stats_categorical = json.loads(df.describe(include='O').loc[[\n",
    "        'count', 'unique'\n",
    "    ]].to_json())\n",
    "    stats_numeric = json.loads(df.describe().loc[[\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]].to_json())\n",
    "\n",
    "    weights = json.loads(df['deliveryid'].groupby([ df[label] for label in LABELS ]).agg(['count']).to_json())\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    STATS = json.dumps(obj={\n",
    "            'columns': {\n",
    "                'all': columns.tolist(),\n",
    "                'categorical': list(stats_categorical.keys()),\n",
    "                'numeric': list(stats_numeric.keys())\n",
    "            },\n",
    "            'stats': { **stats_numeric , **stats_categorical },\n",
    "            'weights': { **weights }\n",
    "        }, indent=4)\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(S3_BUCKET).put_object(Key=DATA_STATS_FILE_KEY, Body=STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hours(startHour=None, numberOfHours=6):\n",
    "    numberOfHours = numberOfHours % 25\n",
    "    if startHour is None:\n",
    "        startHour = datetime.datetime.now().hour\n",
    "    hours = [\"{:02d}\".format( (hour + 24) % 24 ) for hour in range(startHour, startHour - numberOfHours, -1)]\n",
    "    \n",
    "    return hours\n",
    "\n",
    "# print(get_hours(23, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_url(row):\n",
    "    url = str(row['pub_as_pageurl'])\n",
    "    url = url.replace('https://', '')\n",
    "    url = url.replace('http://', '')\n",
    "    url = url.replace('www.', '')\n",
    "\n",
    "    return url\n",
    "\n",
    "def transform_and_save_file(files, destination, is_train=True):\n",
    "    \n",
    "    CONFIG = config.get_config()\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    df = pd.concat((pd.read_csv('s3://{}/{}'.format(S3_BUCKET, f), sep=CSV_SEPARATOR, compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes) for f in files))\n",
    "\n",
    "    df = df[['deliveryid',\n",
    "             'dayofweek',\n",
    "             'hour',\n",
    "             'pub_sspid',\n",
    "             'pub_as_adspaceid',\n",
    "             'pub_as_domain',\n",
    "             'pub_as_dimensions',\n",
    "             'pub_as_position',\n",
    "             'device_os',\n",
    "             'device_model',\n",
    "             'user_ip',\n",
    "             'user_market',\n",
    "             'user_city',\n",
    "              LABEL]]\n",
    "    missing_values = {\n",
    "        'deliveryid': '0',\n",
    "        'dayofweek': 0,\n",
    "        'hour': 0,\n",
    "        'pub_sspid': '0',\n",
    "        'pub_as_adspaceid': '0',\n",
    "        'pub_as_domain': '0',\n",
    "        'pub_as_dimensions': '0',\n",
    "        'pub_as_position': '0',\n",
    "        'device_os': '0',\n",
    "        'device_model': '0',\n",
    "        'user_ip': '0',\n",
    "        'user_market': '0',\n",
    "        'user_city': '0',\n",
    "         LABEL: 0\n",
    "    }\n",
    "    df = df.fillna(value=missing_values)\n",
    "\n",
    "    print('Read file')\n",
    "    df['dayofweek_hour'] = df['dayofweek']*24 + df['hour']\n",
    "    print('Added dayofweek_hour')\n",
    "    df['pub_as_domain'] = df['pub_as_domain'].astype(str).str.lower()\n",
    "    df['user_city'] = df['user_city'].astype(str).str.lower()\n",
    "    df['domain_position'] = df['pub_as_domain'].astype(str) + df['pub_as_position'].astype(str)\n",
    "    print('Added domain_position')\n",
    "#     df['pub_as_pageurl'] = df.apply(lambda row: sanitize_url(row), axis=1)\n",
    "#     print('Sanitized pub_as_pageurl')\n",
    "    if is_train:\n",
    "        df = df.apply(lambda x: x.mask(x.map(x.value_counts()) < 100, '0') if x.name in ['pub_sspid', 'pub_accountid', 'pub_as_siteid', 'pub_as_adspaceid', 'pub_as_domain', 'pub_as_pageurl', 'pub_as_dimensions', 'pub_as_position', 'device_os', 'device_model', 'user_market', 'user_city'] else x)\n",
    "    print('Removed low frequent')\n",
    "    total = float(df['deliveryid'].count())\n",
    "    positive = float(df[df[LABEL] == 1].count()[LABEL])\n",
    "    positive_weight = 0.5 * (total / positive)\n",
    "    negative_weight = 0.5 * (total / (total - positive))\n",
    "    print('Positive weight {}'.format(positive_weight))\n",
    "    print('Negative weight {}'.format(negative_weight))\n",
    "    def get_weight(row):\n",
    "        if int(row[LABEL]) == 1:\n",
    "            return positive_weight\n",
    "        return negative_weight\n",
    "    df['weight'] = df.apply (lambda row: get_weight(row), axis=1)\n",
    "#     df['weight'] = positive_weight\n",
    "#     df[df['imp_6'] == 0]['weight'] = negative_weight\n",
    "    print('Added weight')\n",
    "#     df.head()\n",
    "\n",
    "#     df = df.drop(['dayofweek', 'hour', 'pub_as_position', 'pub_as_domain'], axis=1)\n",
    "#     print('Dropped columns')\n",
    "    new_data = df.to_csv(sep=CSV_SEPARATOR, index=False, na_rep=\"null\")\n",
    "    new_data = gzip.compress(bytes(new_data, 'utf-8'))\n",
    "    io_data = io.BytesIO(new_data)\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_fileobj(io_data, S3_BUCKET, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files names separated for training and evaluation\n",
      "CPU times: user 1.32 s, sys: 41.1 ms, total: 1.36 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SRC_BUCKET = 'wsbidder'\n",
    "DST_BUCKET = 'wsbidder'\n",
    "\n",
    "SRC_PREFIX = 'tsv/etl/imp-pred-service-v1/imppredservice_training_data'\n",
    "DST_PREFIX = 'trainer_predict_imp/data'\n",
    "\n",
    "delete_files(DST_BUCKET, DST_PREFIX + '/train_click')\n",
    "delete_files(DST_BUCKET, DST_PREFIX + '/eval_click')\n",
    "\n",
    "eval_files_date = str( datetime.date.today() - datetime.timedelta(1) )\n",
    "filter_days = [ str( datetime.date.today() - datetime.timedelta(i + 1) ) for i in range(8) ]\n",
    "filter_hours = get_hours(23, 24)\n",
    "\n",
    "all_files = get_files(SRC_BUCKET, SRC_PREFIX, filter_days, filter_hours, ext='.gz')\n",
    "\n",
    "train_files = [ file_path for file_path in all_files if 'd={}'.format(eval_files_date) not in file_path ]\n",
    "eval_files = [ file_path for file_path in all_files if 'd={}'.format(eval_files_date) in file_path ]\n",
    "print('Files names separated for training and evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training file...0 - 999\n",
      "Read file\n",
      "Added dayofweek_hour\n",
      "Added domain_position\n",
      "Removed low frequent\n",
      "Positive weight 403.3071705426357\n",
      "Negative weight 0.5006206443635629\n",
      "Added weight\n",
      "Finished Creating training file 0 - 999\n",
      "Creating training file...1000 - 1999\n",
      "Read file\n",
      "Added dayofweek_hour\n",
      "Added domain_position\n",
      "Removed low frequent\n",
      "Positive weight 419.42261904761904\n",
      "Negative weight 0.5005967689225479\n",
      "Added weight\n",
      "Finished Creating training file 1000 - 1999\n",
      "Creating training file...2000 - 2016\n",
      "Read file\n",
      "Added dayofweek_hour\n",
      "Added domain_position\n",
      "Removed low frequent\n",
      "Positive weight 553.7727272727273\n",
      "Negative weight 0.5004518567203418\n",
      "Added weight\n",
      "Finished Creating training file 2000 - 2016\n",
      "Creating eval file...\n",
      "Read file\n",
      "Added dayofweek_hour\n",
      "Added domain_position\n",
      "Removed low frequent\n",
      "Positive weight 348.11777108433733\n",
      "Negative weight 0.5007191807231839\n",
      "Added weight\n",
      "Finished Creating eval file\n",
      "CPU times: user 6min 7s, sys: 13.4 s, total: 6min 20s\n",
      "Wall time: 8min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "l = len(train_files)\n",
    "step=1000\n",
    "for start in range(0, l, step):\n",
    "    end = min(start + step - 1, l)\n",
    "    print('Creating training file...{} - {}'.format(start, end))\n",
    "    transform_and_save_file(train_files[start: end], '{}/train_click/train_{}_{}.csv.gz'.format(DST_PREFIX, start, end))\n",
    "    print('Finished Creating training file {} - {}'.format(start, end))\n",
    "print('Creating eval file...')\n",
    "transform_and_save_file(eval_files, '{}/eval_click/eval.csv.gz'.format(DST_PREFIX), is_train=False)\n",
    "print('Finished Creating eval file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.7 s, sys: 2.89 s, total: 59.6 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "new_train_files = [ obj.key for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/train_click/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "# print(new_train_files)\n",
    "calculate_stats_and_upload_to_s3([ 's3://{}/{}/train_click/{}'.format(DST_BUCKET, DST_PREFIX, os.path.basename(filename)) for filename in new_train_files ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
