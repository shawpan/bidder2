{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (4.40.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import boto3\n",
    "import random\n",
    "import os\n",
    "import config\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gzip\n",
    "import io\n",
    "import random as rand\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required settings\n",
    "NUM_OF_DAYS = 2\n",
    "MAX_DATASET_FILES = 10\n",
    "MAX_DATASET_FILES_FOR_STATS = 10\n",
    "TRAIN_EVAL_RATIO = 0.9\n",
    "WEIGHT_COLUMN = 'weight'\n",
    "S3_BUCKET = 'wsbidder'\n",
    "CSV_SEPARATOR = '\\t'\n",
    "DATA_STATS_FILE_KEY = 'trainer_predict_bid/data/stats2.json'\n",
    "SRC_BUCKET = 'wsbidder'\n",
    "DST_BUCKET = 'wsbidder'\n",
    "SRC_PREFIX = 'tsv/etl/imp-pred-service-v1/imppredservice_training_data'\n",
    "DST_PREFIX = 'trainer_predict_bid/data'\n",
    "REQUIRED_COLUMNS = {\n",
    "        'deliveryid': '0',\n",
    "        'dayofweek': 0,\n",
    "        'hour': 0,\n",
    "        'pub_sspid': '0',\n",
    "        'pub_as_adspaceid': '0',\n",
    "        'pub_as_domain': '0',\n",
    "        'pub_as_dimensions': '0',\n",
    "        'pub_as_position': '0',\n",
    "        'pub_as_viewrate': 0.0,\n",
    "        'device_os': '0',\n",
    "        'device_model': '0',\n",
    "        'user_ip': '0',\n",
    "        'user_market': '0',\n",
    "        'user_city': '0',\n",
    "        'user_id' : '0',\n",
    "        'pub_as_iabcategoryid': 'IAB24',\n",
    "        'req_auctiontype': 0,\n",
    "        'price': 0.0,\n",
    "        'advcostcpm': 0.0,\n",
    "        'won': 0,\n",
    "        'targetbid': 0\n",
    "    }\n",
    "MINIMUM_FREQUENCY = 100\n",
    "FILTER_LOW_FREQUENCY_COLUMNS = ['pub_sspid', 'pub_accountid', 'pub_as_siteid', 'pub_as_adspaceid', \n",
    "                              'pub_as_domain', 'pub_as_pageurl', 'pub_as_dimensions', 'pub_as_iabcategoryid',\n",
    "                              'pub_as_position', 'device_os', 'device_model', 'user_market', 'user_city',\n",
    "                              'user_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods\n",
    "\n",
    "def delete_files(bucket, prefixes):\n",
    "    for prefix in prefixes:\n",
    "        for obj in boto3.resource('s3').Bucket(bucket).objects.filter(Prefix=prefix).all():\n",
    "            obj.delete()\n",
    "\n",
    "def get_files(bucket, prefix, days=[], hours=[], ext='.csv'):\n",
    "    all_files = []\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    for day in days:\n",
    "        for hour in hours:\n",
    "            _prefix = '{}/d={}/h={}/'.format(prefix, day, hour)\n",
    "            new_files = [ obj.key for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=_prefix).all() if obj.key.endswith(ext) ]\n",
    "            all_files = all_files + new_files\n",
    "            \n",
    "    return all_files\n",
    "\n",
    "def get_hours(startHour=None, numberOfHours=6):\n",
    "    numberOfHours = numberOfHours % 25\n",
    "    if startHour is None:\n",
    "        startHour = datetime.datetime.now().hour\n",
    "    hours = [\"{:02d}\".format( (hour + 24) % 24 ) for hour in range(startHour, startHour - numberOfHours, -1)]\n",
    "    \n",
    "    return hours\n",
    "\n",
    "def sanitize_url(row):\n",
    "    url = str(row['pub_as_pageurl'])\n",
    "    url = url.replace('https://', '')\n",
    "    url = url.replace('http://', '')\n",
    "    url = url.replace('www.', '')\n",
    "\n",
    "    return url\n",
    "\n",
    "def clean_dataset_location():\n",
    "    delete_files(DST_BUCKET, DST_PREFIX + '/train')\n",
    "    delete_files(DST_BUCKET, DST_PREFIX + '/eval')\n",
    "\n",
    "def create_dataset(files, name):\n",
    "    l = len(files)\n",
    "#     print('Total files...{}'.format(l))\n",
    "\n",
    "    for file in tqdm_notebook(files):\n",
    "        destination = '{}/{}/{}'.format(DST_PREFIX, name, os.path.basename(file) )\n",
    "#         print('Creating file...{}'.format(destination))\n",
    "        transform_and_save_file([file], destination)\n",
    "#         print('Finished Creating file {}'.format(destination))\n",
    "\n",
    "def transform_df_columns_for_dataset(df):\n",
    "    df['pub_as_domain'] = df['pub_as_domain'].astype(str).str.lower()\n",
    "    df['user_city'] = df['user_city'].astype(str).str.lower()\n",
    "    df['domain_position'] = df['pub_as_domain'].astype(str) + df['pub_as_position'].astype(str)\n",
    "#     df[WEIGHT_COLUMN] = 1.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_df_columns_for_stats(df):\n",
    "    df['pub_as_viewrate'] = df['pub_as_viewrate'].astype(float)\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    df['advcostcpm'] = df['advcostcpm'].astype(float)\n",
    "    df[WEIGHT_COLUMN] = df[WEIGHT_COLUMN].astype(float)\n",
    "    df['targetbid'] = df['targetbid'].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_and_save_file(files, destination, is_train=True):\n",
    "    if len(files) == 0:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        if len(files) > 1:\n",
    "            df = pd.concat(\n",
    "                    (pd.read_csv('s3://{}/{}'.format(S3_BUCKET, f), \n",
    "                         sep=CSV_SEPARATOR, compression='gzip', \n",
    "                         na_values=[\"null\", \"\\\\N\"]) for f in files)\n",
    "            )\n",
    "        else:\n",
    "            df = pd.read_csv('s3://{}/{}'.format(S3_BUCKET, files[0]), \n",
    "                         sep=CSV_SEPARATOR, compression='gzip', \n",
    "                         na_values=[\"null\", \"\\\\N\"])\n",
    "#         print(\"Number of instances {}\".format(df.shape[0]))\n",
    "    except:\n",
    "#         print('Error reading files')\n",
    "#         print(files)\n",
    "        return\n",
    "    df = df[list(REQUIRED_COLUMNS.keys())]\n",
    "    missing_values = REQUIRED_COLUMNS\n",
    "    df = df.fillna(value=missing_values)\n",
    "    df = transform_df_columns_for_dataset(df)\n",
    "    \n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "#     print('Read file')\n",
    "#     df = df[df['user_market'] == '75']\n",
    "#     df['dayofweek_hour'] = df['dayofweek']*24 + df['hour']\n",
    "#     print('Added dayofweek_hour')\n",
    "#     print('Added domain_position')\n",
    "#     df['pub_as_pageurl'] = df.apply(lambda row: sanitize_url(row), axis=1)\n",
    "#     print('Sanitized pub_as_pageurl')\n",
    "    if is_train:\n",
    "        df = df.apply(\n",
    "            lambda x: x.mask(x.map(x.value_counts()) < MINIMUM_FREQUENCY, '0') \n",
    "                if x.name in FILTER_LOW_FREQUENCY_COLUMNS\n",
    "                else x\n",
    "        )\n",
    "#     print('Removed low frequent')\n",
    "\n",
    "#     df = df.drop(['dayofweek', 'hour', 'pub_as_position', 'pub_as_domain'], axis=1)\n",
    "#     print('Dropped columns')\n",
    "    new_data = df.to_csv(sep=CSV_SEPARATOR, index=False, na_rep=\"null\")\n",
    "    new_data = gzip.compress(bytes(new_data, 'utf-8'))\n",
    "    io_data = io.BytesIO(new_data)\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.upload_fileobj(io_data, S3_BUCKET, destination)\n",
    "    except:\n",
    "        s3.delete_object(Bucket=S3_BUCKET, Key=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files names separated for training and evaluation\n",
      "Deleted unnecessary files\n",
      "CPU times: user 519 ms, sys: 20 ms, total: 539 ms\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Last day and available hours of today\n",
    "filter_days = [ str( datetime.date.today() - datetime.timedelta(i) ) for i in range(NUM_OF_DAYS) ]\n",
    "filter_hours = get_hours(23, 24)\n",
    "\n",
    "all_files = get_files(SRC_BUCKET, SRC_PREFIX, filter_days, filter_hours, ext='.gz')\n",
    "rand.shuffle(all_files)\n",
    "\n",
    "# all_files = all_files[:int(len(all_files) * 0.1)]\n",
    "all_files = all_files[:MAX_DATASET_FILES]\n",
    "train_length = int(len(all_files) * TRAIN_EVAL_RATIO)\n",
    "train_files = all_files[:train_length]\n",
    "eval_files = all_files[train_length:]\n",
    "\n",
    "train_files_basenames = [ os.path.basename(file) for file in train_files ]\n",
    "eval_files_basenames = [ os.path.basename(file) for file in eval_files ]\n",
    "print('Files names separated for training and evaluation')\n",
    "\n",
    "existing_train_files = [ os.path.basename(obj.key) for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/train/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "existing_eval_files = [ os.path.basename(obj.key) for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/eval/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "\n",
    "delete_files_train = [ \"{}/train/{}\".format(DST_PREFIX, file) \n",
    "                          for file in existing_train_files if file not in train_files_basenames ]\n",
    "delete_files_eval = [ \"{}/eval/{}\".format(DST_PREFIX, file) \n",
    "                         for file in existing_eval_files if file not in eval_files_basenames ]\n",
    "\n",
    "train_files = [ file for file in train_files if os.path.basename(file) not in existing_train_files ]\n",
    "eval_files = [ file for file in eval_files if os.path.basename(file) not in existing_eval_files ]\n",
    "\n",
    "\n",
    "delete_files(DST_BUCKET, delete_files_train)\n",
    "delete_files(DST_BUCKET, delete_files_eval)\n",
    "\n",
    "print(\"Deleted unnecessary files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:43: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d10119b42247af9f831e968f829752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Creating train files\n",
      "Creating eval files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd2879ccadb44da8469ce888f4c0169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Creating eval files\n",
      "CPU times: user 2min 34s, sys: 2.62 s, total: 2min 36s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Creating train files')\n",
    "create_dataset(train_files, 'train')\n",
    "print('Finished Creating train files')\n",
    "\n",
    "print('Creating eval files')\n",
    "create_dataset(eval_files, 'eval')\n",
    "print('Finished Creating eval files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats_and_upload_to_s3(all_files):\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    df = pd.concat(\n",
    "        (pd.read_csv(f, sep=CSV_SEPARATOR, \n",
    "                     compression='gzip', \n",
    "                     na_values=[\"null\", \"\\\\N\"], \n",
    "                     dtype=dtypes) for f in all_files)\n",
    "    )\n",
    "    df = transform_df_columns_for_stats(df)\n",
    "    stats_categorical = json.loads(df.describe(include='O').loc[[\n",
    "        'count', 'unique'\n",
    "    ]].to_json())\n",
    "    stats_numeric = json.loads(df.describe().loc[[\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]].to_json())\n",
    "\n",
    "#     weights = json.loads(df['deliveryid'].groupby([ df[label] for label in ['won'] ]).agg(['count']).to_json())\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    STATS = json.dumps(obj={\n",
    "            'columns': {\n",
    "                'all': columns.tolist(),\n",
    "                'categorical': list(stats_categorical.keys()),\n",
    "                'numeric': list(stats_numeric.keys())\n",
    "            },\n",
    "            'stats': { **stats_numeric , **stats_categorical }\n",
    "        }, indent=4)\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(S3_BUCKET).put_object(Key=DATA_STATS_FILE_KEY, Body=STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 1.38 s, total: 32.5 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "new_train_files = [ obj.key for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/train/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "# print(new_train_files)\n",
    "l = len(new_train_files)\n",
    "rand.shuffle(new_train_files)\n",
    "new_train_files = new_train_files[:min(max(MAX_DATASET_FILES_FOR_STATS,100),l)]\n",
    "file_paths = [ 's3://{}/{}/train/{}'.format(DST_BUCKET, DST_PREFIX, os.path.basename(filename)) for filename in new_train_files ]\n",
    "# print(file_paths)\n",
    "calculate_stats_and_upload_to_s3(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG = config.get_config()\n",
    "# dtypes = config.get_types_of_attributes()\n",
    "# df = pd.concat((pd.read_csv(f, sep=CONFIG['CSV_SEPARATOR'], compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes) for f in file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pub_as_viewrate'] = df['pub_as_viewrate'].astype(float)\n",
    "# print(df['pub_as_viewrate'].std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
