{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import boto3\n",
    "import random\n",
    "import os\n",
    "import config\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_object(src_bucket_name, src_object_name,\n",
    "                dest_bucket_name, dest_object_name=None):\n",
    "    \"\"\"Copy an Amazon S3 bucket object\n",
    "\n",
    "    :param src_bucket_name: string\n",
    "    :param src_object_name: string\n",
    "    :param dest_bucket_name: string. Must already exist.\n",
    "    :param dest_object_name: string. If dest bucket/object exists, it is\n",
    "    overwritten. Default: src_object_name\n",
    "    :return: True if object was copied, otherwise False\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct source bucket/object parameter\n",
    "    copy_source = {'Bucket': src_bucket_name, 'Key': src_object_name}\n",
    "    if dest_object_name is None:\n",
    "        dest_object_name = src_object_name\n",
    "\n",
    "    # Copy the object\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.copy_object(CopySource=copy_source, Bucket=dest_bucket_name,\n",
    "                       Key=dest_object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def delete_files(bucket, prefix):\n",
    "    for obj in boto3.resource('s3').Bucket(bucket).objects.filter(Prefix=prefix).all():\n",
    "        obj.delete()\n",
    "\n",
    "def copy_files(src_bucket, dst_bucket, files, dst_prefix):\n",
    "    for file in files:\n",
    "        copy_object(src_bucket, file, dst_bucket, '{}/{}'.format(dst_prefix, os.path.basename(file)))\n",
    "        \n",
    "def get_files(bucket, prefix, days=[], hours=[], ext='.csv'):\n",
    "    all_files = []\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    for day in days:\n",
    "        for hour in hours:\n",
    "            _prefix = '{}/d={}/h={}/'.format(prefix, day, hour)\n",
    "            new_files = [ obj.key for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=_prefix).all() if obj.key.endswith(ext) ]\n",
    "            all_files = all_files + new_files\n",
    "            \n",
    "    return all_files\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats_and_upload_to_s3(all_files):\n",
    "    CONFIG = config.get_config()\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    df = pd.concat((pd.read_csv(f, sep=CONFIG['CSV_SEPARATOR'], compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes) for f in all_files))\n",
    "    stats_categorical = json.loads(df.describe(include='O').loc[[\n",
    "        'count', 'unique'\n",
    "    ]].to_json())\n",
    "    stats_numeric = json.loads(df.describe().loc[[\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]].to_json())\n",
    "\n",
    "    weights = json.loads(df['deliveryid'].groupby([ df[label] for label in CONFIG['PREDICT_IMP_LABELS'] ]).agg(['count']).to_json())\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    STATS = json.dumps(obj={\n",
    "            'columns': {\n",
    "                'all': columns.tolist(),\n",
    "                'categorical': list(stats_categorical.keys()),\n",
    "                'numeric': list(stats_numeric.keys())\n",
    "            },\n",
    "            'stats': { **stats_numeric , **stats_categorical },\n",
    "            'weights': { **weights }\n",
    "        }, indent=4)\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(CONFIG['S3_BUCKET']).put_object(Key=CONFIG['DATA_STATS_FILE_KEY'], Body=STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight_column_to_dataset(all_files):\n",
    "    CONFIG = config.get_config()\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    df = pd.concat((pd.read_csv('s3://{}/{}'.format(CONFIG['S3_BUCKET'], f), sep=CONFIG['CSV_SEPARATOR'], compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes) for f in all_files))\n",
    "    weights = json.loads(df['deliveryid'].groupby([ df[label] for label in CONFIG['PREDICT_IMP_LABELS'] ]).agg(['count']).to_json())\n",
    "    total = float(df['deliveryid'].count())\n",
    "    def get_weight(row):\n",
    "        labels = CONFIG['PREDICT_IMP_LABELS']\n",
    "        key = ','.join([ str(row[label]) for label in labels ])\n",
    "        if len(labels) > 1:\n",
    "            key = '[{}]'.format(key)\n",
    "        freq = 1.0\n",
    "        if key in weights['count']:\n",
    "            freq = weights['count'][key]\n",
    "#         prob = freq / total\n",
    "#         target_prob = 1. / ( 2.0 ** len(labels) )\n",
    "#         return target_prob / prob\n",
    "        return total / ( ( 2.0 ** len(labels) ) * freq )\n",
    "    s3 = boto3.resource('s3')\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv('s3://{}/{}'.format(CONFIG['S3_BUCKET'], f), sep=CONFIG['CSV_SEPARATOR'], compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes)\n",
    "        df['weight'] = df.apply (lambda row: get_weight(row), axis=1)\n",
    "        new_data = df.to_csv(sep=CONFIG['CSV_SEPARATOR'], index=False, na_rep=\"null\")\n",
    "        new_data = gzip.compress(bytes(new_data, 'utf-8'))\n",
    "        s3.Bucket(CONFIG['S3_BUCKET']).put_object(Key=f, Body=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hours(startHour=None, numberOfHours=6):\n",
    "    numberOfHours = numberOfHours % 25\n",
    "    if startHour is None:\n",
    "        startHour = datetime.datetime.now().hour\n",
    "    hours = [\"{:02d}\".format( (hour + 24) % 24 ) for hour in range(startHour, startHour - numberOfHours, -1)]\n",
    "    \n",
    "    return hours\n",
    "\n",
    "# print(get_hours(23, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files separated\n",
      "Added weight\n"
     ]
    }
   ],
   "source": [
    "SRC_BUCKET = 'wsbidder'\n",
    "DST_BUCKET = 'wsbidder'\n",
    "\n",
    "SRC_PREFIX = 'tsv/etl/imp-pred-service-v1/imppredservice_training_data'\n",
    "# SRC_PREFIX = 'trainer_predict_imp/data2'\n",
    "DST_PREFIX = 'trainer_predict_imp/data'\n",
    "\n",
    "delete_files(DST_BUCKET, DST_PREFIX + '/train')\n",
    "delete_files(DST_BUCKET, DST_PREFIX + '/eval')\n",
    "\n",
    "eval_files_date = str( datetime.date.today() - datetime.timedelta(1) )\n",
    "filter_days = [ str( datetime.date.today() - datetime.timedelta(i + 2) ) for i in range(7) ]\n",
    "filter_hours = get_hours(23, 24)\n",
    "\n",
    "all_files = get_files(SRC_BUCKET, SRC_PREFIX, filter_days, filter_hours, ext='.gz')\n",
    "\n",
    "# number_of_files = len(all_files)\n",
    "\n",
    "# # random.shuffle(all_files)\n",
    "# train_length = int(number_of_files * 0.6)\n",
    "# train_files = all_files[:train_length]\n",
    "# eval_files = all_files[train_length + 1:]\n",
    "\n",
    "train_files = [ file_path for file_path in all_files if 'd={}'.format(eval_files_date) not in file_path ]\n",
    "eval_files = [ file_path for file_path in all_files if 'd={}'.format(eval_files_date) in file_path ]\n",
    "\n",
    "print('Files separated')\n",
    "add_weight_column_to_dataset(all_files)\n",
    "print('Added weight')\n",
    "\n",
    "copy_files(SRC_BUCKET, DST_BUCKET, train_files, DST_PREFIX + '/train')\n",
    "copy_files(SRC_BUCKET, DST_BUCKET, eval_files, DST_PREFIX + '/eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([ 's3://{}/{}/train/{}'.format(DST_BUCKET, DST_PREFIX, os.path.basename(filename)) for filename in train_files ])\n",
    "calculate_stats_and_upload_to_s3([ 's3://{}/{}/train/{}'.format(DST_BUCKET, DST_PREFIX, os.path.basename(filename)) for filename in train_files ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
