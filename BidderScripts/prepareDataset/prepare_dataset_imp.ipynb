{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import boto3\n",
    "import random\n",
    "import os\n",
    "import config\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gzip\n",
    "import io\n",
    "import random as rand\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required settings\n",
    "NUM_OF_DAYS = 30\n",
    "NUM_OF_DAYS_FOR_STATS = 7\n",
    "FILES_PER_DAY = 288\n",
    "MAX_DATASET_FILES = NUM_OF_DAYS * FILES_PER_DAY\n",
    "MAX_DATASET_FILES_FOR_STATS = NUM_OF_DAYS_FOR_STATS * FILES_PER_DAY\n",
    "EMPTY_FILES = 0\n",
    "TRAIN_EVAL_RATIO = 0.9\n",
    "WEIGHT_COLUMN = 'weight'\n",
    "S3_BUCKET = 'wsbidder'\n",
    "CSV_SEPARATOR = '\\t'\n",
    "DATA_STATS_FILE_KEY = 'trainer_predict_imp/data/stats2.json'\n",
    "SRC_BUCKET = 'wsbidder'\n",
    "DST_BUCKET = 'wsbidder'\n",
    "SRC_PREFIX = 'tsv/etl/imp-pred-service-v1/imppredservice_training_data'\n",
    "DST_PREFIX = 'trainer_predict_imp/data'\n",
    "PREDICT_IMP_LABELS = ['imp_4', 'imp_6', 'imp_7']\n",
    "PREDICT_IMP_LABELS_DISPLAY = ['imp_1', 'imp_6', 'imp_7', 'imp_12', 'imp_13', 'imp_14']\n",
    "PREDICT_IMP_LABELS_VIDEO = ['imp_2', 'imp_3', 'imp_4', 'imp_5', 'imp_8', 'imp_9', 'imp_10', 'imp_11']\n",
    "PREDICT_IMP_TYPES_DISPLAY = ['1', '6', '7', '12', '13', '14']\n",
    "PREDICT_IMP_TYPES_VIDEO = ['2', '3', '4', '8', '9', '10', '11']\n",
    "\n",
    "REQUIRED_COLUMNS = {\n",
    "        'deliveryid': '0',\n",
    "        'dayofweek': 0,\n",
    "        'hour': 0,\n",
    "        'pub_sspid': '0',\n",
    "        'pub_as_adspaceid': '0',\n",
    "        'pub_as_domain': '0',\n",
    "        'pub_as_dimensions': '0',\n",
    "        'pub_as_position': '0',\n",
    "        'pub_as_viewrate': 0.0,\n",
    "        'device_os': '0',\n",
    "        'device_model': '0',\n",
    "        'user_ip': '0',\n",
    "        'user_market': '0',\n",
    "        'user_city': '0',\n",
    "        'ad_imptype': '0',\n",
    "        'user_id' : '0',\n",
    "        'ad_formatid' : '0',\n",
    "        'pub_as_iabcategoryid': 'IAB24',\n",
    "        'req_auctiontype': 0,\n",
    "        'imp_0': 0,\n",
    "        'imp_1': 0,\n",
    "        'imp_2': 0,\n",
    "        'imp_3': 0,\n",
    "        'imp_4': 0,\n",
    "        'imp_5': 0,\n",
    "        'imp_6': 0,\n",
    "        'imp_7': 0,\n",
    "        'imp_8': 0,\n",
    "        'imp_9': 0,\n",
    "        'imp_10': 0,\n",
    "        'imp_11': 0,\n",
    "        'imp_12': 0,\n",
    "        'imp_13': 0,\n",
    "        'imp_14': 0\n",
    "    }\n",
    "MINIMUM_FREQUENCY = 100\n",
    "FILTER_LOW_FREQUENCY_COLUMNS = ['pub_sspid', 'pub_accountid', 'pub_as_siteid', 'pub_as_adspaceid', \n",
    "                              'pub_as_domain', 'pub_as_pageurl', 'pub_as_dimensions', 'pub_as_iabcategoryid',\n",
    "                              'pub_as_position', 'device_os', 'device_model', 'user_market', 'user_city',\n",
    "                              'user_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods\n",
    "\n",
    "def delete_files(bucket, prefixes):\n",
    "    for prefix in prefixes:\n",
    "        for obj in boto3.resource('s3').Bucket(bucket).objects.filter(Prefix=prefix).all():\n",
    "            obj.delete()\n",
    "\n",
    "def get_files(bucket, prefix, days=[], hours=[], ext='.csv'):\n",
    "    all_files = []\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    for day in days:\n",
    "        for hour in hours:\n",
    "            _prefix = '{}/d={}/h={}/'.format(prefix, day, hour)\n",
    "            new_files = [ obj.key for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=_prefix).all() if obj.key.endswith(ext) ]\n",
    "            all_files = all_files + new_files\n",
    "            \n",
    "    return all_files\n",
    "\n",
    "def get_hours(startHour=None, numberOfHours=6):\n",
    "    numberOfHours = numberOfHours % 25\n",
    "    if startHour is None:\n",
    "        startHour = datetime.datetime.now().hour\n",
    "    hours = [\"{:02d}\".format( (hour + 24) % 24 ) for hour in range(startHour, startHour - numberOfHours, -1)]\n",
    "    \n",
    "    return hours\n",
    "\n",
    "def sanitize_url(row):\n",
    "    url = str(row['pub_as_pageurl'])\n",
    "    url = url.replace('https://', '')\n",
    "    url = url.replace('http://', '')\n",
    "    url = url.replace('www.', '')\n",
    "\n",
    "    return url\n",
    "\n",
    "def clean_dataset_location():\n",
    "    delete_files(DST_BUCKET, DST_PREFIX + '/train')\n",
    "    delete_files(DST_BUCKET, DST_PREFIX + '/eval')\n",
    "\n",
    "def create_dataset(files, name):\n",
    "    EMPTY_FILES = 0\n",
    "    l = len(files)\n",
    "#     print('Total files...{}'.format(l))\n",
    "\n",
    "    for file in tqdm_notebook(files):\n",
    "        destination = '{}/{}/{}'.format(DST_PREFIX, name, os.path.basename(file) )\n",
    "#         print('Creating file...{}'.format(destination))\n",
    "        transform_and_save_file([file], destination)\n",
    "#         print('Finished Creating file {}'.format(destination))\n",
    "\n",
    "def transform_df_columns_for_dataset(df):\n",
    "    df = df[df['imp_0'] == 1 ]\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df['dayofweek_hour'] = df['dayofweek']*24 + df['hour']\n",
    "    df['pub_as_domain'] = df['pub_as_domain'].astype(str).str.lower()\n",
    "    df['user_city'] = df['user_city'].astype(str).str.lower()\n",
    "    df['domain_position'] = df['pub_as_domain'].astype(str) + df['pub_as_position'].astype(str)\n",
    "    df[WEIGHT_COLUMN] = 1.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_df_columns_for_stats(df):\n",
    "    df['pub_as_viewrate'] = df['pub_as_viewrate'].astype(float)\n",
    "    df['domain_position'] = df['domain_position'].astype(str)\n",
    "    df['ad_formatid'] = df['ad_formatid'].astype(str)\n",
    "    df[WEIGHT_COLUMN] = df[WEIGHT_COLUMN].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_files(DST_BUCKET, DST_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_and_save_file(files, destination, is_train=True):\n",
    "    if len(files) == 0:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        if len(files) > 1:\n",
    "            df = pd.concat(\n",
    "                    (pd.read_csv('s3://{}/{}'.format(S3_BUCKET, f), \n",
    "                         sep=CSV_SEPARATOR, compression='gzip', \n",
    "                         na_values=[\"null\", \"\\\\N\"]) for f in files)\n",
    "            )\n",
    "        else:\n",
    "            df = pd.read_csv('s3://{}/{}'.format(S3_BUCKET, files[0]), \n",
    "                         sep=CSV_SEPARATOR, compression='gzip', \n",
    "                         na_values=[\"null\", \"\\\\N\"])\n",
    "#         print(\"Number of instances {}\".format(df.shape[0]))\n",
    "    except:\n",
    "#         print('Error reading files')\n",
    "#         print(files)\n",
    "        return\n",
    "    df = df[list(REQUIRED_COLUMNS.keys())]\n",
    "    missing_values = REQUIRED_COLUMNS\n",
    "    df = df.fillna(value=missing_values)\n",
    "    df = transform_df_columns_for_dataset(df)\n",
    "    \n",
    "    if df.empty:\n",
    "        EMPTY_FILES = EMPTY_FILES + 1\n",
    "        return\n",
    "\n",
    "#     print('Read file')\n",
    "#     df = df[df['user_market'] == '75']\n",
    "#     df['dayofweek_hour'] = df['dayofweek']*24 + df['hour']\n",
    "#     print('Added dayofweek_hour')\n",
    "#     print('Added domain_position')\n",
    "#     df['pub_as_pageurl'] = df.apply(lambda row: sanitize_url(row), axis=1)\n",
    "#     print('Sanitized pub_as_pageurl')\n",
    "    if is_train:\n",
    "        df = df.apply(\n",
    "            lambda x: x.mask(x.map(x.value_counts()) < MINIMUM_FREQUENCY, '0') \n",
    "                if x.name in FILTER_LOW_FREQUENCY_COLUMNS\n",
    "                else x\n",
    "        )\n",
    "#     print('Removed low frequent')\n",
    "\n",
    "#     df = df.drop(['dayofweek', 'hour', 'pub_as_position', 'pub_as_domain'], axis=1)\n",
    "#     print('Dropped columns')\n",
    "    new_data = df.to_csv(sep=CSV_SEPARATOR, index=False, na_rep=\"null\")\n",
    "    new_data = gzip.compress(bytes(new_data, 'utf-8'))\n",
    "    io_data = io.BytesIO(new_data)\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.upload_fileobj(io_data, S3_BUCKET, destination)\n",
    "    except:\n",
    "        s3.delete_object(Bucket=S3_BUCKET, Key=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Last day and available hours of today\n",
    "filter_days = [ str( datetime.date.today() - datetime.timedelta(i) ) for i in range(NUM_OF_DAYS) ]\n",
    "filter_hours = get_hours(23, 24)\n",
    "\n",
    "all_files = get_files(SRC_BUCKET, SRC_PREFIX, filter_days, filter_hours, ext='.gz')\n",
    "rand.shuffle(all_files)\n",
    "\n",
    "# all_files = all_files[:int(len(all_files) * 0.1)]\n",
    "all_files = all_files[:MAX_DATASET_FILES]\n",
    "train_length = int(len(all_files) * TRAIN_EVAL_RATIO)\n",
    "train_files = all_files[:train_length]\n",
    "eval_files = all_files[train_length:]\n",
    "\n",
    "train_files_basenames = [ os.path.basename(file) for file in train_files ]\n",
    "eval_files_basenames = [ os.path.basename(file) for file in eval_files ]\n",
    "print('Files names separated for training and evaluation')\n",
    "\n",
    "existing_train_files = [ os.path.basename(obj.key) for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/train/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "existing_eval_files = [ os.path.basename(obj.key) for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/eval/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "\n",
    "delete_files_train = [ \"{}/train/{}\".format(DST_PREFIX, file) \n",
    "                          for file in existing_train_files if file not in train_files_basenames ]\n",
    "delete_files_eval = [ \"{}/eval/{}\".format(DST_PREFIX, file) \n",
    "                         for file in existing_eval_files if file not in eval_files_basenames ]\n",
    "\n",
    "train_files = [ file for file in train_files if os.path.basename(file) not in existing_train_files ]\n",
    "eval_files = [ file for file in eval_files if os.path.basename(file) not in existing_eval_files ]\n",
    "\n",
    "\n",
    "delete_files(DST_BUCKET, delete_files_train)\n",
    "delete_files(DST_BUCKET, delete_files_eval)\n",
    "\n",
    "print(\"Deleted unnecessary files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Creating train files')\n",
    "create_dataset(train_files, 'train')\n",
    "print('Finished Creating train files')\n",
    "print('{} empty files'.format(EMPTY_FILES))\n",
    "\n",
    "print('Creating eval files')\n",
    "create_dataset(eval_files, 'eval')\n",
    "print('Finished Creating eval files')\n",
    "print('{} empty files'.format(EMPTY_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight_column_to_dataset(all_files, weights_info):\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    freq_display = weights_info['freq_display']\n",
    "    freq_video = weights_info['freq_video']\n",
    "    total_display = weights_info['total_display']\n",
    "    total_video = weights_info['total_video']\n",
    "    \n",
    "    def get_weight_for(row, frequencies, total):\n",
    "        labels = PREDICT_IMP_LABELS\n",
    "        label_len = len(list(set(PREDICT_IMP_LABELS_DISPLAY) & set(PREDICT_IMP_LABELS))) if row['ad_imptype'] in PREDICT_IMP_LABELS_DISPLAY else len(PREDICT_IMP_LABELS)\n",
    "        key = ','.join([ str(row[label]) for label in labels ])\n",
    "        if len(labels) > 1:\n",
    "            key = '[{}]'.format(key)\n",
    "        freq = 1.0\n",
    "        if key in frequencies['count']:\n",
    "            freq = frequencies['count'][key]\n",
    "#         prob = freq / total\n",
    "#         target_prob = 1. / ( 2.0 ** len(labels) )\n",
    "#         return target_prob / prob\n",
    "        combination_len = len(frequencies['count'])\n",
    "#         return (total + 1.0) / ( ( 2.0 ** label_len ) * freq + 1.0 )\n",
    "        return (total + 1.0) / ( combination_len * (freq + 1.0) )\n",
    "\n",
    "    def get_weight(row):\n",
    "        if row['ad_imptype'] in PREDICT_IMP_LABELS_DISPLAY:\n",
    "            return get_weight_for(row, freq_display, total_display)\n",
    "        return get_weight_for(row, freq_video, total_video)\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    for f in tqdm_notebook(all_files):\n",
    "        df = pd.read_csv(f, sep=CSV_SEPARATOR, compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes)\n",
    "        df[WEIGHT_COLUMN] = df.apply (lambda row: get_weight(row), axis=1)\n",
    "        new_data = df.to_csv(sep=CSV_SEPARATOR, index=False, na_rep=\"null\")\n",
    "        new_data = gzip.compress(bytes(new_data, 'utf-8'))\n",
    "        s3.Bucket(S3_BUCKET).put_object(Key=f[len('s3://{}/'.format(S3_BUCKET)):], Body=new_data)\n",
    "        \n",
    "def calculate_stats_and_upload_to_s3(all_files):\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    df = pd.concat(\n",
    "        (pd.read_csv(f, sep=CSV_SEPARATOR, \n",
    "                     compression='gzip', \n",
    "                     na_values=[\"null\", \"\\\\N\"], \n",
    "                     dtype=dtypes) for f in all_files)\n",
    "    )\n",
    "    df = transform_df_columns_for_stats(df)\n",
    "    stats_categorical = json.loads(df.describe(include='O').loc[[\n",
    "        'count', 'unique'\n",
    "    ]].to_json())\n",
    "    stats_numeric = json.loads(df.describe().loc[[\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]].to_json())\n",
    "\n",
    "    frequencies_display_ads = json.loads(\n",
    "        df[ df.ad_imptype.isin(PREDICT_IMP_TYPES_DISPLAY) ][['deliveryid'] + PREDICT_IMP_LABELS ].groupby(\n",
    "            PREDICT_IMP_LABELS\n",
    "        ).agg(['count'])['deliveryid'].to_json()\n",
    "    )\n",
    "    total_display_ads = sum([ int(frequencies_display_ads['count'][key]) for key in  frequencies_display_ads['count']])\n",
    "    \n",
    "    frequencies_video_ads = json.loads(\n",
    "        df[ df.ad_imptype.isin(PREDICT_IMP_TYPES_VIDEO) ][['deliveryid'] + PREDICT_IMP_LABELS ].groupby(\n",
    "            PREDICT_IMP_LABELS\n",
    "        ).agg(['count'])['deliveryid'].to_json()\n",
    "    )\n",
    "    total_video_ads = sum([ int(frequencies_video_ads['count'][key]) for key in  frequencies_video_ads['count']])\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    STATS = json.dumps(obj={\n",
    "            'columns': {\n",
    "                'all': columns.tolist(),\n",
    "                'categorical': list(stats_categorical.keys()),\n",
    "                'numeric': list(stats_numeric.keys())\n",
    "            },\n",
    "            'stats': { **stats_numeric , **stats_categorical },\n",
    "            'frequencies_display_ads': { **frequencies_display_ads },\n",
    "            'total_display_ads' : total_display_ads,\n",
    "            'frequencies_video_ads': { **frequencies_video_ads },\n",
    "            'total_video_ads' : total_video_ads\n",
    "        }, indent=4)\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(S3_BUCKET).put_object(Key=DATA_STATS_FILE_KEY, Body=STATS)\n",
    "    \n",
    "    return {\n",
    "        'freq_display': frequencies_display_ads,\n",
    "        'freq_video': frequencies_video_ads,\n",
    "        'total_display': total_display_ads,\n",
    "        'total_video': total_video_ads\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DST_BUCKET' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DST_BUCKET' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "def get_file_paths(files, train_or_eval):\n",
    "    return [ 's3://{}/{}/{}/{}'.format(DST_BUCKET, DST_PREFIX, train_or_eval, os.path.basename(filename)) for filename in files ]\n",
    "new_train_files = [ obj.key for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/train/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "new_eval_files = [ obj.key for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/eval/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "\n",
    "# print(new_train_files)\n",
    "l = len(new_train_files)\n",
    "rand.shuffle(new_train_files)\n",
    "stats_files = new_train_files[:min(max(MAX_DATASET_FILES_FOR_STATS,100),l)]\n",
    "stats_file_paths = get_file_paths(stats_files, 'train')\n",
    "\n",
    "print(\"Started statistics calculation\")\n",
    "stats = calculate_stats_and_upload_to_s3(stats_file_paths)\n",
    "print(\"Ended statistics calculation\")\n",
    "\n",
    "train_file_paths = get_file_paths(new_train_files, 'train')\n",
    "eval_file_paths = get_file_paths(new_eval_files, 'eval')\n",
    "\n",
    "print(\"Started adding weight to train files\")\n",
    "add_weight_column_to_dataset(train_file_paths, stats)\n",
    "print(\"Ended adding weight to train files\")\n",
    "\n",
    "print(\"Started adding weight to eval files\")\n",
    "add_weight_column_to_dataset(eval_file_paths, stats)\n",
    "print(\"Ended adding weight to eval files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG = config.get_config()\n",
    "# dtypes = config.get_types_of_attributes()\n",
    "# df = pd.concat((pd.read_csv(f, sep=CONFIG['CSV_SEPARATOR'], compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes) for f in file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pub_as_viewrate'] = df['pub_as_viewrate'].astype(float)\n",
    "# print(df['pub_as_viewrate'].std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
