{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/55/fd9170ba08a1a64a18a7f8a18f088037316f2a41be04d2fe6ece5a653e8f/tqdm-4.43.0-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 12.2MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.43.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import boto3\n",
    "import random\n",
    "import os\n",
    "import config\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gzip\n",
    "import io\n",
    "import random as rand\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required settings\n",
    "NUM_OF_DAYS = 30\n",
    "NUM_OF_DAYS_FOR_STATS = 7\n",
    "FILES_PER_DAY = 288\n",
    "MAX_DATASET_FILES = NUM_OF_DAYS * FILES_PER_DAY\n",
    "MAX_DATASET_FILES_FOR_STATS = NUM_OF_DAYS_FOR_STATS * FILES_PER_DAY\n",
    "EMPTY_FILES = 0\n",
    "TRAIN_EVAL_RATIO = 0.9\n",
    "WEIGHT_COLUMN = 'weight'\n",
    "S3_BUCKET = 'wsbidder'\n",
    "CSV_SEPARATOR = '\\t'\n",
    "DATA_STATS_FILE_KEY = 'trainer_predict_click/data/stats2.json'\n",
    "SRC_BUCKET = 'wsbidder'\n",
    "DST_BUCKET = 'wsbidder'\n",
    "SRC_PREFIX = 'tsv/etl/imp-pred-service-v1/imppredservice_training_data'\n",
    "DST_PREFIX = 'trainer_predict_click/data'\n",
    "PREDICT_IMP_LABELS = ['click']\n",
    "\n",
    "REQUIRED_COLUMNS = {\n",
    "        'deliveryid': '0',\n",
    "        'dayofweek': 0,\n",
    "        'hour': 0,\n",
    "        'pub_sspid': '0',\n",
    "        'pub_as_adspaceid': '0',\n",
    "        'pub_as_domain': '0',\n",
    "        'pub_as_dimensions': '0',\n",
    "        'pub_as_position': '0',\n",
    "        'pub_as_viewrate': 0.0,\n",
    "        'device_os': '0',\n",
    "        'device_model': '0',\n",
    "        'user_ip': '0',\n",
    "        'user_market': '0',\n",
    "        'user_city': '0',\n",
    "        'ad_imptype': '0',\n",
    "        'user_id' : '0',\n",
    "        'ad_formatid' : '0',\n",
    "        'pub_as_iabcategoryid': 'IAB24',\n",
    "        'req_auctiontype': 0,\n",
    "        'imp_0': 0,\n",
    "        'click': 0\n",
    "    }\n",
    "MINIMUM_FREQUENCY = 100\n",
    "FILTER_LOW_FREQUENCY_COLUMNS = ['pub_sspid', 'pub_accountid', 'pub_as_siteid', 'pub_as_adspaceid', \n",
    "                              'pub_as_domain', 'pub_as_pageurl', 'pub_as_dimensions', 'pub_as_iabcategoryid',\n",
    "                              'pub_as_position', 'device_os', 'device_model', 'user_market', 'user_city',\n",
    "                              'user_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods\n",
    "\n",
    "def delete_files(bucket, prefixes):\n",
    "    for prefix in prefixes:\n",
    "        for obj in boto3.resource('s3').Bucket(bucket).objects.filter(Prefix=prefix).all():\n",
    "            obj.delete()\n",
    "\n",
    "def get_files(bucket, prefix, days=[], hours=[], ext='.csv'):\n",
    "    all_files = []\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    for day in days:\n",
    "        for hour in hours:\n",
    "            _prefix = '{}/d={}/h={}/'.format(prefix, day, hour)\n",
    "            new_files = [ obj.key for obj in s3_resource.Bucket(bucket).objects.filter(Prefix=_prefix).all() if obj.key.endswith(ext) ]\n",
    "            all_files = all_files + new_files\n",
    "            \n",
    "    return all_files\n",
    "\n",
    "def get_hours(startHour=None, numberOfHours=6):\n",
    "    numberOfHours = numberOfHours % 25\n",
    "    if startHour is None:\n",
    "        startHour = datetime.datetime.now().hour\n",
    "    hours = [\"{:02d}\".format( (hour + 24) % 24 ) for hour in range(startHour, startHour - numberOfHours, -1)]\n",
    "    \n",
    "    return hours\n",
    "\n",
    "def sanitize_url(row):\n",
    "    url = str(row['pub_as_pageurl'])\n",
    "    url = url.replace('https://', '')\n",
    "    url = url.replace('http://', '')\n",
    "    url = url.replace('www.', '')\n",
    "\n",
    "    return url\n",
    "\n",
    "def clean_dataset_location():\n",
    "    delete_files(DST_BUCKET, DST_PREFIX + '/train')\n",
    "    delete_files(DST_BUCKET, DST_PREFIX + '/eval')\n",
    "\n",
    "def create_dataset(files, name):\n",
    "    EMPTY_FILES = 0\n",
    "    l = len(files)\n",
    "#     print('Total files...{}'.format(l))\n",
    "\n",
    "    for file in tqdm_notebook(files):\n",
    "        destination = '{}/{}/{}'.format(DST_PREFIX, name, os.path.basename(file) )\n",
    "#         print('Creating file...{}'.format(destination))\n",
    "        transform_and_save_file([file], destination)\n",
    "#         print('Finished Creating file {}'.format(destination))\n",
    "\n",
    "def transform_df_columns_for_dataset(df):\n",
    "    df = df[df['imp_0'] == 1 ]\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df['dayofweek_hour'] = df['dayofweek']*24 + df['hour']\n",
    "    df['pub_as_domain'] = df['pub_as_domain'].astype(str).str.lower()\n",
    "    df['user_city'] = df['user_city'].astype(str).str.lower()\n",
    "    df['domain_position'] = df['pub_as_domain'].astype(str) + df['pub_as_position'].astype(str)\n",
    "    df[WEIGHT_COLUMN] = 1.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_df_columns_for_stats(df):\n",
    "    df['pub_as_viewrate'] = df['pub_as_viewrate'].astype(float)\n",
    "    df['domain_position'] = df['domain_position'].astype(str)\n",
    "    df['ad_formatid'] = df['ad_formatid'].astype(str)\n",
    "    df[WEIGHT_COLUMN] = df[WEIGHT_COLUMN].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_files(DST_BUCKET, DST_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_and_save_file(files, destination, is_train=True):\n",
    "    if len(files) == 0:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        if len(files) > 1:\n",
    "            df = pd.concat(\n",
    "                    (pd.read_csv('s3://{}/{}'.format(S3_BUCKET, f), \n",
    "                         sep=CSV_SEPARATOR, compression='gzip', \n",
    "                         na_values=[\"null\", \"\\\\N\"]) for f in files)\n",
    "            )\n",
    "        else:\n",
    "            df = pd.read_csv('s3://{}/{}'.format(S3_BUCKET, files[0]), \n",
    "                         sep=CSV_SEPARATOR, compression='gzip', \n",
    "                         na_values=[\"null\", \"\\\\N\"])\n",
    "#         print(\"Number of instances {}\".format(df.shape[0]))\n",
    "    except:\n",
    "#         print('Error reading files')\n",
    "#         print(files)\n",
    "        return\n",
    "    df = df[list(REQUIRED_COLUMNS.keys())]\n",
    "    missing_values = REQUIRED_COLUMNS\n",
    "    df = df.fillna(value=missing_values)\n",
    "    df = transform_df_columns_for_dataset(df)\n",
    "    \n",
    "    if df.empty:\n",
    "        EMPTY_FILES = EMPTY_FILES + 1\n",
    "        return\n",
    "\n",
    "#     print('Read file')\n",
    "#     df = df[df['user_market'] == '75']\n",
    "#     df['dayofweek_hour'] = df['dayofweek']*24 + df['hour']\n",
    "#     print('Added dayofweek_hour')\n",
    "#     print('Added domain_position')\n",
    "#     df['pub_as_pageurl'] = df.apply(lambda row: sanitize_url(row), axis=1)\n",
    "#     print('Sanitized pub_as_pageurl')\n",
    "    if is_train:\n",
    "        df = df.apply(\n",
    "            lambda x: x.mask(x.map(x.value_counts()) < MINIMUM_FREQUENCY, '0') \n",
    "                if x.name in FILTER_LOW_FREQUENCY_COLUMNS\n",
    "                else x\n",
    "        )\n",
    "#     print('Removed low frequent')\n",
    "\n",
    "#     df = df.drop(['dayofweek', 'hour', 'pub_as_position', 'pub_as_domain'], axis=1)\n",
    "#     print('Dropped columns')\n",
    "    new_data = df.to_csv(sep=CSV_SEPARATOR, index=False, na_rep=\"null\")\n",
    "    new_data = gzip.compress(bytes(new_data, 'utf-8'))\n",
    "    io_data = io.BytesIO(new_data)\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        s3.upload_fileobj(io_data, S3_BUCKET, destination)\n",
    "    except:\n",
    "        s3.delete_object(Bucket=S3_BUCKET, Key=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files names separated for training and evaluation\n",
      "Deleted unnecessary files\n",
      "CPU times: user 1min 2s, sys: 1.17 s, total: 1min 3s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# Last day and available hours of today\n",
    "filter_days = [ str( datetime.date.today() - datetime.timedelta(i) ) for i in range(NUM_OF_DAYS) ]\n",
    "filter_hours = get_hours(23, 24)\n",
    "\n",
    "all_files = get_files(SRC_BUCKET, SRC_PREFIX, filter_days, filter_hours, ext='.gz')\n",
    "rand.shuffle(all_files)\n",
    "\n",
    "# all_files = all_files[:int(len(all_files) * 0.1)]\n",
    "all_files = all_files[:MAX_DATASET_FILES]\n",
    "train_length = int(len(all_files) * TRAIN_EVAL_RATIO)\n",
    "train_files = all_files[:train_length]\n",
    "eval_files = all_files[train_length:]\n",
    "\n",
    "train_files_basenames = [ os.path.basename(file) for file in train_files ]\n",
    "eval_files_basenames = [ os.path.basename(file) for file in eval_files ]\n",
    "print('Files names separated for training and evaluation')\n",
    "\n",
    "existing_train_files = [ os.path.basename(obj.key) for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/train/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "existing_eval_files = [ os.path.basename(obj.key) for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/eval/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "\n",
    "delete_files_train = [ \"{}/train/{}\".format(DST_PREFIX, file) \n",
    "                          for file in existing_train_files if file not in train_files_basenames ]\n",
    "delete_files_eval = [ \"{}/eval/{}\".format(DST_PREFIX, file) \n",
    "                         for file in existing_eval_files if file not in eval_files_basenames ]\n",
    "\n",
    "train_files = [ file for file in train_files if os.path.basename(file) not in existing_train_files ]\n",
    "eval_files = [ file for file in eval_files if os.path.basename(file) not in existing_eval_files ]\n",
    "\n",
    "\n",
    "delete_files(DST_BUCKET, delete_files_train)\n",
    "delete_files(DST_BUCKET, delete_files_eval)\n",
    "\n",
    "print(\"Deleted unnecessary files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:44: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caefd60046904c168661704edbfba3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1705.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/magics/execution.py:1238: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code, glob, local_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Creating train files\n",
      "0 empty files\n",
      "Creating eval files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2574b61d6d9467f9ba2d6712c1c693f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=778.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished Creating eval files\n",
      "0 empty files\n",
      "CPU times: user 44min 2s, sys: 11.3 s, total: 44min 13s\n",
      "Wall time: 54min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Creating train files')\n",
    "create_dataset(train_files, 'train')\n",
    "print('Finished Creating train files')\n",
    "print('{} empty files'.format(EMPTY_FILES))\n",
    "\n",
    "print('Creating eval files')\n",
    "create_dataset(eval_files, 'eval')\n",
    "print('Finished Creating eval files')\n",
    "print('{} empty files'.format(EMPTY_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight_column_to_dataset(all_files, weights_info):\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    total = weights_info['total']\n",
    "    positive = weights_info['positive']\n",
    "    \n",
    "    def get_weight_for(row, positive, total):\n",
    "        weight_for_0 = total / (2*(total - positive + 1))\n",
    "        weight_for_1 = total / (2*(positive + 1))\n",
    "#         prob = freq / total\n",
    "#         target_prob = 1. / ( 2.0 ** len(labels) )\n",
    "#         return target_prob / prob\n",
    "        return weight_for_1 if int(row['click']) == 1 else weight_for_0\n",
    "\n",
    "    def get_weight(row):\n",
    "        return get_weight_for(row, positive, total)\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    for f in tqdm_notebook(all_files):\n",
    "        df = pd.read_csv(f, sep=CSV_SEPARATOR, compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes)\n",
    "        df[WEIGHT_COLUMN] = df.apply (lambda row: get_weight(row), axis=1)\n",
    "        new_data = df.to_csv(sep=CSV_SEPARATOR, index=False, na_rep=\"null\")\n",
    "        new_data = gzip.compress(bytes(new_data, 'utf-8'))\n",
    "        s3.Bucket(S3_BUCKET).put_object(Key=f[len('s3://{}/'.format(S3_BUCKET)):], Body=new_data)\n",
    "        \n",
    "def calculate_stats_and_upload_to_s3(all_files):\n",
    "    dtypes = config.get_types_of_attributes()\n",
    "    df = pd.concat(\n",
    "        (pd.read_csv(f, sep=CSV_SEPARATOR, \n",
    "                     compression='gzip', \n",
    "                     na_values=[\"null\", \"\\\\N\"], \n",
    "                     dtype=dtypes) for f in all_files)\n",
    "    )\n",
    "    df = transform_df_columns_for_stats(df)\n",
    "    stats_categorical = json.loads(df.describe(include='O').loc[[\n",
    "        'count', 'unique'\n",
    "    ]].to_json())\n",
    "    stats_numeric = json.loads(df.describe().loc[[\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]].to_json())\n",
    "\n",
    "    total = float(df['deliveryid'].count())\n",
    "    positive = float(df[df[PREDICT_IMP_LABELS[0]] == 1].count()[PREDICT_IMP_LABELS[0]])\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    STATS = json.dumps(obj={\n",
    "            'columns': {\n",
    "                'all': columns.tolist(),\n",
    "                'categorical': list(stats_categorical.keys()),\n",
    "                'numeric': list(stats_numeric.keys())\n",
    "            },\n",
    "            'stats': { **stats_numeric , **stats_categorical },\n",
    "            'positive': positive,\n",
    "            'total' : total\n",
    "        }, indent=4)\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(S3_BUCKET).put_object(Key=DATA_STATS_FILE_KEY, Body=STATS)\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'positive': positive\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started statistics calculation\n",
      "Ended statistics calculation\n",
      "Started adding weight to train files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28355456a72e4cd6ba317bc6649f9c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7622.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ended adding weight to train files\n",
      "Started adding weight to eval files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087c7a649f25481c8c96427bb7ae817d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=848.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ended adding weight to eval files\n",
      "CPU times: user 30min 30s, sys: 8.46 s, total: 30min 38s\n",
      "Wall time: 46min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "def get_file_paths(files, train_or_eval):\n",
    "    return [ 's3://{}/{}/{}/{}'.format(DST_BUCKET, DST_PREFIX, train_or_eval, os.path.basename(filename)) for filename in files ]\n",
    "new_train_files = [ obj.key for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/train/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "new_eval_files = [ obj.key for obj in s3_resource.Bucket(DST_BUCKET).objects.filter(Prefix='{}/eval/'.format(DST_PREFIX)).all() if obj.key.endswith('.gz') ]\n",
    "\n",
    "# print(new_train_files)\n",
    "l = len(new_train_files)\n",
    "rand.shuffle(new_train_files)\n",
    "stats_files = new_train_files[:min(max(MAX_DATASET_FILES_FOR_STATS,100),l)]\n",
    "stats_file_paths = get_file_paths(stats_files, 'train')\n",
    "\n",
    "print(\"Started statistics calculation\")\n",
    "stats = calculate_stats_and_upload_to_s3(stats_file_paths)\n",
    "print(\"Ended statistics calculation\")\n",
    "\n",
    "train_file_paths = get_file_paths(new_train_files, 'train')\n",
    "eval_file_paths = get_file_paths(new_eval_files, 'eval')\n",
    "\n",
    "print(\"Started adding weight to train files\")\n",
    "add_weight_column_to_dataset(train_file_paths, stats)\n",
    "print(\"Ended adding weight to train files\")\n",
    "\n",
    "print(\"Started adding weight to eval files\")\n",
    "add_weight_column_to_dataset(eval_file_paths, stats)\n",
    "print(\"Ended adding weight to eval files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG = config.get_config()\n",
    "# dtypes = config.get_types_of_attributes()\n",
    "# df = pd.concat((pd.read_csv(f, sep=CONFIG['CSV_SEPARATOR'], compression='gzip', na_values=[\"null\", \"\\\\N\"], dtype=dtypes) for f in file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pub_as_viewrate'] = df['pub_as_viewrate'].astype(float)\n",
    "# print(df['pub_as_viewrate'].std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
